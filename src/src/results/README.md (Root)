# Co-Sign Language: AI-Powered Eye Communication System for ALS Patients

**Developer:** Sharl Michael Effat Habib  
**Affiliation:** Independent Researcher â€“ Cornell University (Ongoing ARXIV Publication)  
**Languages & Frameworks:** Python, TensorFlow, Keras, OpenCV  

---

## ğŸ©º Overview
Co-Sign Language is an AI-based assistive system designed to translate **eye movements into speech** for ALS and motor-impaired patients.  
It combines **Vision Transformers (ViT)**, **Kalman filtering**, and a **Non-Intentionality Suppression (NIS)** algorithm to distinguish intentional from unintentional eye movements.

- ğŸ§© **98% gaze detection accuracy**  
- âš¡ **Real-time communication** at 20+ words/min  
- ğŸ’¡ **Cost-effective hardware**: <$50 smart glasses prototype  
- ğŸ§‘â€ğŸ’» Research-focused, aiming for scalable, inclusive assistive AI

---

## âš™ï¸ Key Features
- Vision Transformer model for grayscale eye images  
- Kalman filtering for stabilizing gaze detection  
- NIS-based algorithm to filter involuntary blinks  
- Lightweight pipeline suitable for embedded systems  
- Data augmentation to increase robustness

---

## ğŸ§¬ Model Architecture
- Input: `(224, 224, 1)` grayscale eye image  
- Patch size: `7x7` â†’ 1024 patches  
- Transformer layers: 12  
- Multi-head attention: 8 heads  
- MLP head units: [128, 64]  
- Dropout: 0.1  
- Output: 6-class softmax classification for different intentional commands

---

## ğŸ“Š Results
| Metric      | Value        |
|------------|-------------|
| Accuracy    | 98%          |
| Precision   | 96%          |
| Recall      | 97%          |
| Latency     | <100ms/frame |

> **Note:** Full dataset and trained model weights are not publicly shared yet due to ongoing ARXIV publication.

---

## ğŸ“ Repository Structure

